{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: \n",
    "\n",
    "<div class=\"line-block\"><code>1   It does not print the fruit at the correct index, why is the returned result wrong?</code><br />\n",
    "<code>2   How could this be fixed?</code></div>\n",
    "\n",
    "### Solution: \n",
    " The issue with the code is that all the fruits all in a set and the problem with a set is that it is not in an order. So, for returning the correct fruit name for each fruit id, we should use a List instead of a Set. (As shown below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_fruit(fruit_id: int, fruits: list) -> str:\n",
    "    \n",
    "    if fruit_id < 0 or fruit_id >= len(fruits):\n",
    "        raise ValueError(f\"Fruit with id {fruit_id} does not exist\")\n",
    "    return fruits[fruit_id]\n",
    "\n",
    "fruits = [\"apple\", \"orange\", \"melon\", \"kiwi\", \"strawberry\"]\n",
    "name1 = id_to_fruit(1, fruits)\n",
    "name3 = id_to_fruit(3, fruits)\n",
    "name4 = id_to_fruit(4, fruits)\n",
    "print(name1,name3,name4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "\n",
    "<div class=\"line-block\"><code>1   Can you spot the obvious error?</code><br />\n",
    "<code>2   After fixing the obvious error it is still wrong, how can this be fixed?</code></div>\n",
    "\n",
    "### Solution: \n",
    "The obvious error is:\n",
    "\n",
    " ```coords[:, 0], coords[:, 1], ... = coords[:, 1], coords[:, 1], ... ```\n",
    "\n",
    "it should be:\n",
    "\n",
    " ```coords[:, 0], coords[:, 1], ... = coords[:, 1], coords[:, 0], ....```\n",
    "\n",
    "Also, by printing out the \"swapped_croods\", we would get all the Y coordinates as all the X coordinates, which is wrong. (regarding that we only needed them to be flipped.)\n",
    "And as a solution, we can copy each coords on the other side, so that we wouldn't have the same columns of Y(s) where the X column sits. (as shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def swap(coords: np.ndarray):\n",
    "    coords[:, 0], coords[:, 1], coords[:, 2], coords[:, 3] = coords[:, 1].copy(), coords[:, 0].copy(), coords[:, 3].copy(), coords[:, 2].copy()\n",
    "    return coords\n",
    "\n",
    "coords = np.array([[10, 5, 15, 6, 0],\n",
    "                   [11, 3, 13, 6, 0],\n",
    "                   [5, 3, 13, 6, 1],\n",
    "                   [4, 4, 13, 6, 1],\n",
    "                   [6, 5, 13, 16, 1]])\n",
    "swapped_coords = swap(coords)\n",
    "\n",
    "print(swapped_coords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercies 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the 'pandas' library to simplify the process of reading the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(csv_file_path: str):\n",
    "    # Read data using pandas\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract precision and recall columns\n",
    "    precision = df['precision'].values\n",
    "    recall = df['recall'].values\n",
    "\n",
    "    plt.plot(recall, precision)\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exerecise 4: <br>\n",
    "The issue arises from the mismatch in the size of the labels provided to the loss function when training the discriminator. This happens because the generator generates samples with a different batch size than the real samples.\n",
    "\n",
    "To fix this, we should adjust the size of the generated samples labels to match the batch size of the generator <br>\n",
    "As for the cosmetic bug, the code contains unnecessary try-except blocks and duplicated data loading for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "NoneType = type(None)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models import vgg11\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "def train_gan(batch_size: int = 32, num_epochs: int = 100, device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    # Download MNIST dataset\n",
    "    torchvision.datasets.MNIST.resources = [\n",
    "        ('https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbdeb94873'),\n",
    "        ('https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz', 'd53e105ee54ea40749a09fcbcd1e9432'),\n",
    "        ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'),\n",
    "        ('https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c')\n",
    "    ]\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # example data\n",
    "    real_samples, mnist_labels = next(iter(train_loader))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(16):\n",
    "        sub = fig.add_subplot(4, 4, 1 + i)\n",
    "        sub.imshow(real_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "        sub.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(\"Real images\")\n",
    "    display(fig)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Set up training\n",
    "    discriminator = Discriminator().to(device)\n",
    "    generator = Generator().to(device)\n",
    "    lr = 0.0001\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(num_epochs):\n",
    "        for n, (real_samples, mnist_labels) in enumerate(train_loader):\n",
    "\n",
    "            # Data for training the discriminator\n",
    "            real_samples = real_samples.to(device=device)\n",
    "            real_samples_labels = torch.ones((batch_size, 1)).to(device=device)\n",
    "            latent_space_samples = torch.randn((batch_size, 100)).to(device=device)\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            generated_samples_labels = torch.zeros((batch_size, 1)).to(device=device)\n",
    "            all_samples = torch.cat((real_samples, generated_samples))\n",
    "            all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))\n",
    "\n",
    "            # Training the discriminator\n",
    "            discriminator.zero_grad()\n",
    "            output_discriminator = discriminator(all_samples)\n",
    "            loss_discriminator = loss_function(output_discriminator, all_samples_labels)\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            # Data for training the generator\n",
    "            latent_space_samples = torch.randn((batch_size, 100)).to(device=device)\n",
    "\n",
    "            # Training the generator\n",
    "            generator.zero_grad()\n",
    "            generated_samples = generator(latent_space_samples)\n",
    "            output_discriminator_generated = discriminator(generated_samples)\n",
    "            # Adjust the size of generated_samples_labels to match the batch size of the generator\n",
    "            generated_samples_labels = torch.ones((batch_size, 1)).to(device=device)\n",
    "            loss_generator = loss_function(output_discriminator_generated, generated_samples_labels)\n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "            # Show loss and samples generated\n",
    "            if n == batch_size - 1:\n",
    "                name = f\"Generate images\\n Epoch: {epoch} Loss D.: {loss_discriminator:.2f} Loss G.: {loss_generator:.2f}\"\n",
    "                generated_samples = generated_samples.detach().cpu().numpy()\n",
    "                fig = plt.figure()\n",
    "                for i in range(16):\n",
    "                    sub = fig.add_subplot(4, 4, 1 + i)\n",
    "                    sub.imshow(generated_samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "                    sub.axis('off')\n",
    "                fig.suptitle(name)\n",
    "                fig.tight_layout()\n",
    "                clear_output(wait=False)\n",
    "                display(fig)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
